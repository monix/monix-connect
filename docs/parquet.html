<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>Apache Parquet · Monix Connect</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="## Introduction"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="Apache Parquet · Monix Connect"/><meta property="og:type" content="website"/><meta property="og:url" content="https://connect.monix.io/"/><meta property="og:description" content="## Introduction"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://connect.monix.io/img/monix-logo.png"/><link rel="shortcut icon" href="/img/monix-logo.png"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css"/><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/monix-logo.svg" alt="Monix Connect"/><h2 class="headerTitleWithLogo">Monix Connect</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/api/monix/connect/index.html" target="_self">API Docs</a></li><li class="siteNavGroupActive"><a href="/docs/overview" target="_self">Documentation</a></li><li class=""><a href="https://github.com/monix/monix-connect" target="_self">GitHub</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>Documentation</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">Documentation</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/overview">Overview</a></li><li class="navListItem"><a class="navItem" href="/docs/akka">Akka Streams</a></li><li class="navListItem navListItemActive"><a class="navItem" href="/docs/parquet">Apache Parquet</a></li><li class="navListItem"><a class="navItem" href="/docs/dynamodb">AWS DynamoDB</a></li><li class="navListItem"><a class="navItem" href="/docs/s3">AWS S3</a></li><li class="navListItem"><a class="navItem" href="/docs/elasticsearch">Elasticsearch</a></li><li class="navListItem"><a class="navItem" href="/docs/gcs">Google Cloud Storage</a></li><li class="navListItem"><a class="navItem" href="/docs/hdfs">HDFS</a></li><li class="navListItem"><a class="navItem" href="/docs/mongodb">MongoDB</a></li><li class="navListItem"><a class="navItem" href="/docs/redis">Redis</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer docsContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1 id="__docusaurus" class="postHeaderTitle">Apache Parquet</h1></header><article><div><span><h2><a class="anchor" aria-hidden="true" id="introduction"></a><a href="#introduction" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introduction</h2>
<p><a href="http://parquet.apache.org/">Apache Parquet</a> is a columnar storage format that provides the advantages of <em>compressed</em>, <em>efficient</em> data representation available to any project in the <em>Hadoop</em> ecosystem.</p>
<p>It has already been proved by multiple projects that have demonstrated the performance impact of applying the right <em>compression</em> and <em>encoding scheme</em> to the data.</p>
<p>Therefore, the <code>monix-parquet</code> <em>connector</em> basically exposes stream integrations for <em>reading</em> and <em>writing</em>  from and into parquet files either in the <em>local system</em>, <em>hdfs</em> or <em>AWS S3</em>.</p>
<h2><a class="anchor" aria-hidden="true" id="set-up"></a><a href="#set-up" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Set up</h2>
<p>Add the following dependency:</p>
<pre><code class="hljs css language-scala">libraryDependencies += <span class="hljs-string">"io.monix"</span> %% <span class="hljs-string">"monix-parquet"</span> % <span class="hljs-string">"0.5.0"</span>
</code></pre>
<h2><a class="anchor" aria-hidden="true" id="getting-started"></a><a href="#getting-started" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Getting started</h2>
<p>The two signatures <code>write</code> and <code>read</code> are built on top of the <em>apache parquet</em> <code>ParquetWriter[T]</code> and <code>ParquetReader[T]</code> respectively, therefore they need an instance of these types to be passed.</p>
<p>The <em>type parameter</em> <code>T</code> represents the data type that is expected to be read or written in the parquet file.
In which it can depend on the parquet implementation chosen, since <code>ParqueReader</code> and <code>ParquetWriter</code> are just the
generic classes but you would need to use the implementation that fits better to your use case.</p>
<p>The examples shown in the following sections uses the subclass <code>AvroParquet</code> in which the type parameter <code>T</code> would need to be a subtype of <code>org.apache.avro.generic.GenericRecord</code>.
For these examples we have created our own schema using <code>org.apache.avro.Schema</code>, however, it is <em>highly recommended</em> to generate them using  one of the existing libraries in the scala ecosystem such like <a href="https://github.com/sksamuel/avro4s">Avro4s</a>, <a href="https://github.com/julianpeeters/avrohugger">Avrohugger</a>, <a href="https://fd4s.github.io/vulcan/docs/modules">Vulcan</a>.</p>
<pre><code class="hljs css language-scala"><span class="hljs-comment">// used to parse from and to `GenericRecord`, in a real world example it would be a subtype of `GenericRecord`.</span>
<span class="hljs-keyword">case</span> <span class="hljs-class"><span class="hljs-keyword">class</span> <span class="hljs-title">Person</span>(<span class="hljs-params">id: <span class="hljs-type">Int</span>, name: <span class="hljs-type">String</span></span>)</span>

<span class="hljs-keyword">import</span> org.apache.avro.<span class="hljs-type">Schema</span>
<span class="hljs-comment">// avro schema associated to the above case class </span>
<span class="hljs-keyword">val</span> schema: <span class="hljs-type">Schema</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">Schema</span>.<span class="hljs-type">Parser</span>().parse(
   <span class="hljs-string">"{\"type\":\"record\",\"name\":\"Person\",\"fields\":[{\"name\":\"id\",\"type\":\"int\"},{\"name\":\"name\",\"type\":\"string\"}]}"</span>)
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="writer"></a><a href="#writer" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Writer</h3>
<p>Now, let's get our hands dirty with an example on how to write from <code>Person</code> into a parquet file.
To do so, we are going to use <code>AvroParquetWriter</code> which expects elements subtype of <code>GenericRecord</code>,
First, we we need a function to convert from our <code>Person</code>s:</p>
<pre><code class="hljs css language-scala"><span class="hljs-keyword">import</span> org.apache.avro.generic.{<span class="hljs-type">GenericRecord</span>, <span class="hljs-type">GenericRecordBuilder</span>}

<span class="hljs-comment">// wouldn't be necessary if we were using a scala avro schema generator (avro4s, avrohugger, ...)</span>
<span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">personToGenericRecord</span></span>(person: <span class="hljs-type">Person</span>): <span class="hljs-type">GenericRecord</span> =
    <span class="hljs-keyword">new</span> <span class="hljs-type">GenericRecordBuilder</span>(schema) <span class="hljs-comment">// using the schema created previously</span>
      .set(<span class="hljs-string">"id"</span>, person.id)
      .set(<span class="hljs-string">"name"</span>, person.name)
      .build()
</code></pre>
<p>And then we can consume an <code>Observable[Person]</code> that will write each of the emitted elements into the specified parquet file.</p>
<pre><code class="hljs css language-scala"><span class="hljs-keyword">import</span> monix.eval.<span class="hljs-type">Task</span>
<span class="hljs-keyword">import</span> monix.reactive.<span class="hljs-type">Observable</span>
<span class="hljs-keyword">import</span> monix.connect.parquet.<span class="hljs-type">ParquetSink</span>
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.<span class="hljs-type">Path</span>
<span class="hljs-keyword">import</span> org.apache.parquet.hadoop.<span class="hljs-type">ParquetWriter</span>
<span class="hljs-keyword">import</span> org.apache.parquet.avro.<span class="hljs-type">AvroParquetWriter</span>
<span class="hljs-keyword">import</span> org.apache.avro.generic.<span class="hljs-type">GenericRecord</span>
<span class="hljs-keyword">import</span> org.apache.hadoop.conf.<span class="hljs-type">Configuration</span>

<span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">Configuration</span>()
<span class="hljs-keyword">val</span> path: <span class="hljs-type">Path</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">Path</span>(<span class="hljs-string">"./writer/example/file.parquet"</span>)
<span class="hljs-keyword">val</span> elements: <span class="hljs-type">List</span>[<span class="hljs-type">Person</span>] = <span class="hljs-type">List</span>(<span class="hljs-type">Person</span>(<span class="hljs-number">1</span>, <span class="hljs-string">"Alice"</span>), <span class="hljs-type">Person</span>(<span class="hljs-number">2</span>, <span class="hljs-string">"Bob"</span>)) 
<span class="hljs-keyword">val</span> parquetWriter: <span class="hljs-type">ParquetWriter</span>[<span class="hljs-type">GenericRecord</span>] = {
  <span class="hljs-type">AvroParquetWriter</span>.builder[<span class="hljs-type">GenericRecord</span>](path)
    .withConf(conf)
    .withSchema(schema)
    .build()
}

<span class="hljs-comment">// returns the number of written records</span>
<span class="hljs-keyword">val</span> t: <span class="hljs-type">Task</span>[<span class="hljs-type">Long</span>] = {
  <span class="hljs-type">Observable</span>
    .fromIterable(elements) <span class="hljs-comment">// Observable[Person]</span>
    .map(_ =&gt; personToGenericRecord(_))
    .consumeWith(<span class="hljs-type">ParquetSink</span>.fromWriterUnsafe(parquetWriter))
}
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="reader"></a><a href="#reader" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Reader</h3>
<p>Again, since we are using a low level api we need to write a function to to convert from <code>GenericRecord</code> to a <code>Person</code>:</p>
<pre><code class="hljs css language-scala"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">recordToPerson</span></span>(record: <span class="hljs-type">GenericRecord</span>): <span class="hljs-type">Person</span> =
    <span class="hljs-type">Person</span>(record.get(<span class="hljs-string">"id"</span>).asInstanceOf[<span class="hljs-type">Int</span>], 
           record.get(<span class="hljs-string">"name"</span>).toString)
</code></pre>
<p>Then we will be able to read <em>parquet</em> files as <code>Observable[Person]</code>.</p>
<pre><code class="hljs css language-scala"><span class="hljs-keyword">import</span> monix.connect.parquet.<span class="hljs-type">ParquetSource</span>
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.<span class="hljs-type">Path</span>
<span class="hljs-keyword">import</span> org.apache.parquet.avro.{<span class="hljs-type">AvroParquetReader</span>, <span class="hljs-type">AvroReadSupport</span>}
<span class="hljs-keyword">import</span> org.apache.avro.generic.<span class="hljs-type">GenericRecord</span>
<span class="hljs-keyword">import</span> org.apache.hadoop.conf.<span class="hljs-type">Configuration</span>
<span class="hljs-keyword">import</span> org.apache.parquet.hadoop.util.<span class="hljs-type">HadoopInputFile</span>

<span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">Configuration</span>()
conf.setBoolean(<span class="hljs-type">AvroReadSupport</span>.<span class="hljs-type">AVRO_COMPATIBILITY</span>, <span class="hljs-literal">true</span>)
<span class="hljs-keyword">val</span> path: <span class="hljs-type">Path</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">Path</span>(<span class="hljs-string">"./reader/example/file.parquet"</span>)
<span class="hljs-keyword">val</span> reader: <span class="hljs-type">ParquetReader</span>[<span class="hljs-type">GenericRecord</span>] = {
 <span class="hljs-type">AvroParquetReader</span>
  .builder[<span class="hljs-type">GenericRecord</span>](<span class="hljs-type">HadoopInputFile</span>.fromPath(path, conf))
  .withConf(conf)
  .build()
}

<span class="hljs-keyword">val</span> ob: <span class="hljs-type">Observable</span>[<span class="hljs-type">Person</span>] = <span class="hljs-type">ParquetSource</span>.fromReaderUnsafe(reader).map(recordToPerson)
</code></pre>
<h2><a class="anchor" aria-hidden="true" id="local-testing"></a><a href="#local-testing" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Local testing</h2>
<p>It will depend on the specific use case, as we mentioned earlier in the introductory section it can operate on the <em>local filesystem</em> on <em>hdfs</em> or even in <em>S3</em> (for <em>avro</em> and <em>protobuf</em> parquet sub-modules)</p>
<p>Therefore, depending on the application requirements, the hadoop <code>Configuration</code> class will need to be configured accordingly.</p>
<p><strong>Local:</strong> So far in the examples has been shown how to use it locally, in which in that case it would just be needed to create a plain instance of hadoop configuration, and the <code>Path</code> that would
represent the file in the local system.</p>
<p><strong>Hdfs:</strong> On the other hand, the most common case is to work with parquet files in hdfs, in that case my recommendation is to find specific posts and examples on how to set up your configuration for that.
But on some extend, for setting up the local test environment you would need to use the hadoop minicluster and set the configuration accordingly.
You can check the how to do so in the <code>monix-hdfs</code> documentation.</p>
<p><strong>S3:</strong> Finally, integrating the parequet connector with AWS S3 requires <a href="https://hadoop.apache.org/docs/current/hadoop-aws/tools/hadoop-aws/index.html">specific configuration values</a> to be set. On behalf of configuring it
to run local tests ...
Note that you will also require to spin up a docker container for emulating the AWS S3 service, check how to do so in the <code>monix-s3</code> documentation.</p>
</span></div></article></div><div class="docs-prevnext"><a class="docs-prev button" href="/docs/akka"><span class="arrow-prev">← </span><span>Akka Streams</span></a><a class="docs-next button" href="/docs/dynamodb"><span class="function-name-prevnext">AWS DynamoDB</span><span class="arrow-next"> →</span></a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#introduction">Introduction</a></li><li><a href="#set-up">Set up</a></li><li><a href="#getting-started">Getting started</a><ul class="toc-headings"><li><a href="#writer">Writer</a></li><li><a href="#reader">Reader</a></li></ul></li><li><a href="#local-testing">Local testing</a></li></ul></nav></div><footer class="nav-footer" id="footer"><hr class="separator"/><section class="copyright">Copyright © 2020-2020 The Monix Connect Developers.</section></footer></div></body></html>