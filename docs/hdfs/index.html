<!DOCTYPE html><html lang="en"><head><meta charSet="utf-8"/><meta http-equiv="X-UA-Compatible" content="IE=edge"/><title>HDFS · Monix Connect</title><meta name="viewport" content="width=device-width"/><meta name="generator" content="Docusaurus"/><meta name="description" content="## Introduction"/><meta name="docsearch:language" content="en"/><meta property="og:title" content="HDFS · Monix Connect"/><meta property="og:type" content="website"/><meta property="og:url" content="https://connect.monix.io/"/><meta property="og:description" content="## Introduction"/><meta name="twitter:card" content="summary"/><meta name="twitter:image" content="https://connect.monix.io/img/monix-logo.png"/><link rel="shortcut icon" href="/img/monix-logo.png"/><link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css"/><script src="/js/scrollSpy.js"></script><link rel="stylesheet" href="/css/main.css"/><script src="/js/codetabs.js"></script></head><body class="sideNavVisible separateOnPageNav"><div class="fixedHeaderContainer"><div class="headerWrapper wrapper"><header><a href="/"><img class="logo" src="/img/monix-logo.svg" alt="Monix Connect"/><h2 class="headerTitleWithLogo">Monix Connect</h2></a><div class="navigationWrapper navigationSlider"><nav class="slidingNav"><ul class="nav-site nav-site-internal"><li class=""><a href="/api/monix/connect/index.html" target="_self">API Docs</a></li><li class="siteNavGroupActive"><a href="/docs/overview" target="_self">Documentation</a></li><li class=""><a href="https://github.com/monix/monix-connect" target="_self">GitHub</a></li></ul></nav></div></header></div></div><div class="navPusher"><div class="docMainWrapper wrapper"><div class="docsNavContainer" id="docsNav"><nav class="toc"><div class="toggleNav"><section class="navWrapper wrapper"><div class="navBreadcrumb wrapper"><div class="navToggle" id="navToggler"><div class="hamburger-menu"><div class="line1"></div><div class="line2"></div><div class="line3"></div></div></div><h2><i>›</i><span>Documentation</span></h2><div class="tocToggler" id="tocToggler"><i class="icon-toc"></i></div></div><div class="navGroups"><div class="navGroup"><h3 class="navGroupCategoryTitle">Documentation</h3><ul class=""><li class="navListItem"><a class="navItem" href="/docs/overview">Overview</a></li><li class="navListItem"><a class="navItem" href="/docs/akka">Akka Streams</a></li><li class="navListItem"><a class="navItem" href="/docs/dynamodb">AWS DynamoDB</a></li><li class="navListItem"><a class="navItem" href="/docs/s3">AWS S3</a></li><li class="navListItem"><a class="navItem" href="/docs/gcs">Google Cloud Storage</a></li><li class="navListItem navListItemActive"><a class="navItem" href="/docs/hdfs">HDFS</a></li><li class="navListItem"><a class="navItem" href="/docs/parquet">Apache Parquet</a></li><li class="navListItem"><a class="navItem" href="/docs/redis">Redis</a></li></ul></div></div></section></div><script>
            var coll = document.getElementsByClassName('collapsible');
            var checkActiveCategory = true;
            for (var i = 0; i < coll.length; i++) {
              var links = coll[i].nextElementSibling.getElementsByTagName('*');
              if (checkActiveCategory){
                for (var j = 0; j < links.length; j++) {
                  if (links[j].classList.contains('navListItemActive')){
                    coll[i].nextElementSibling.classList.toggle('hide');
                    coll[i].childNodes[1].classList.toggle('rotate');
                    checkActiveCategory = false;
                    break;
                  }
                }
              }

              coll[i].addEventListener('click', function() {
                var arrow = this.childNodes[1];
                arrow.classList.toggle('rotate');
                var content = this.nextElementSibling;
                content.classList.toggle('hide');
              });
            }

            document.addEventListener('DOMContentLoaded', function() {
              createToggler('#navToggler', '#docsNav', 'docsSliderActive');
              createToggler('#tocToggler', 'body', 'tocActive');

              var headings = document.querySelector('.toc-headings');
              headings && headings.addEventListener('click', function(event) {
                var el = event.target;
                while(el !== headings){
                  if (el.tagName === 'A') {
                    document.body.classList.remove('tocActive');
                    break;
                  } else{
                    el = el.parentNode;
                  }
                }
              }, false);

              function createToggler(togglerSelector, targetSelector, className) {
                var toggler = document.querySelector(togglerSelector);
                var target = document.querySelector(targetSelector);

                if (!toggler) {
                  return;
                }

                toggler.onclick = function(event) {
                  event.preventDefault();

                  target.classList.toggle(className);
                };
              }
            });
        </script></nav></div><div class="container mainContainer docsContainer"><div class="wrapper"><div class="post"><header class="postHeader"><h1 id="__docusaurus" class="postHeaderTitle">HDFS</h1></header><article><div><span><h2><a class="anchor" aria-hidden="true" id="introduction"></a><a href="#introduction" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Introduction</h2>
<p>The <em>Hadoop Distributed File System</em> (<a href="https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html">HDFS</a>) is a distributed file system designed to run on commodity hardware,
also being highly fault-tolerant and providing high throughput access makes it very suitable for applications that have to handle with large data sets.</p>
<p>This connector then allows to <em>read</em> and <em>write</em> hdfs files of any size in a streaming fashion.</p>
<p>The methods to perform these operations are exposed under the object <code>monix.connect.hdfs.Hdfs</code>,
and it is built on top of the the official <em>apache hadoop</em> api.</p>
<h2><a class="anchor" aria-hidden="true" id="dependency"></a><a href="#dependency" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Dependency</h2>
<p>Add the following dependency to get started:</p>
<pre><code class="hljs css language-scala">libraryDependencies += <span class="hljs-string">"io.monix"</span> %% <span class="hljs-string">"monix-hdfs"</span> % <span class="hljs-string">"0.1.0"</span>
</code></pre>
<p>By default the connector uses <em>Hadoop 3.1.1</em>. In case you need a different one you can replace it by excluding <code>org.apache.hadoop</code> from <code>monix-hdfs</code> and add the new one to your library dependencies.</p>
<h2><a class="anchor" aria-hidden="true" id="getting-started"></a><a href="#getting-started" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Getting started</h2>
<h3><a class="anchor" aria-hidden="true" id="configuration"></a><a href="#configuration" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Configuration</h3>
<p>The following import is a common requirement for all those methods defined in the <code>Hdfs</code> object:</p>
<pre><code class="hljs css language-scala"><span class="hljs-keyword">import</span> org.apache.hadoop.fs.<span class="hljs-type">FileSystem</span>
<span class="hljs-comment">//abstract representation of a file system which could be a distributed or a local one.</span>
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.<span class="hljs-type">Path</span>
<span class="hljs-comment">//represents a file or directory in a FileSystem</span>
</code></pre>
<p>Each use case would need different settings to create the hadoop configurations, but
for testing purposes we would just need a plain one:</p>
<pre><code class="hljs css language-scala"><span class="hljs-keyword">import</span> org.apache.hadoop.conf.<span class="hljs-type">Configuration</span>

<span class="hljs-keyword">val</span> hadoopConf = <span class="hljs-keyword">new</span> <span class="hljs-type">Configuration</span>() <span class="hljs-comment">//provides access to the hadoop configurable parameters</span>
<span class="hljs-keyword">val</span> fs: <span class="hljs-type">FileSystem</span> = <span class="hljs-type">FileSystem</span>.get(conf)
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="reader"></a><a href="#reader" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Reader</h3>
<p>Let's start interacting with hdfs, the following example shows how to construct a pipeline that reads from the specified hdfs file.</p>
<p>Normally working with hdfs means that you will be dealing with big data, it makes it difficult or very expensive (if not impossible) to read the whole file at once from a single machine,
therefore the application will read the file in small parts configured by the user that eventually will end with the whole file being processed.</p>
<pre><code class="hljs css language-scala"><span class="hljs-keyword">val</span> sourcePath: <span class="hljs-type">Path</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">Path</span>(<span class="hljs-string">"/source/hdfs/file_source.txt"</span>)
<span class="hljs-keyword">val</span> chunkSize: <span class="hljs-type">Int</span> = <span class="hljs-number">8192</span> <span class="hljs-comment">//size of the chunks to be pulled</span>

<span class="hljs-comment">//once we have the hadoop classes we can create the hdfs monix reader</span>
<span class="hljs-keyword">val</span> ob: <span class="hljs-type">Observable</span>[<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>]] = <span class="hljs-type">Hdfs</span>.read(fs, path, chunkSize)
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="writer"></a><a href="#writer" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Writer</h3>
<p>The following example shows how consume a stream of bytes create a file and writes chunks into it:</p>
<pre><code class="hljs css language-scala"><span class="hljs-keyword">val</span> destinationPath: <span class="hljs-type">Path</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">Path</span>(<span class="hljs-string">"/destination/hdfs/file_dest.txt"</span>)
<span class="hljs-keyword">val</span> hdfsWriter: <span class="hljs-type">Consumer</span>[<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Task</span>[<span class="hljs-type">Long</span>]] = <span class="hljs-type">Hdfs</span>.write(fs, destinationPath) 

<span class="hljs-comment">//eventually it will return the size of the written file</span>
<span class="hljs-keyword">val</span> t: <span class="hljs-type">Task</span>[<span class="hljs-type">Long</span>] = ob.consumeWith(hdfsWriter) 
</code></pre>
<p>It materializes to a <code>Long</code> value that represents the file size.</p>
<p>Note that the hdfs <code>write</code> signature allows different configurations to be passed as parameters such as to
<em>enable overwrite</em> (<code>true</code> by default), <em>replication factor</em> <code>3</code>, the <em>bufferSize</em>  of <code>4096 bytes</code>, <em>blockSize</em> <code>134217728 bytes =~ 128 MB</code>_
and finally a <em>line separator</em> which is not used by default <em>None</em>.</p>
<p>Below example shows an example on how easily can them be tweaked:</p>
<pre><code class="hljs css language-scala"><span class="hljs-keyword">val</span> hdfsWriter: <span class="hljs-type">Consumer</span>[<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Long</span>] = 
   <span class="hljs-type">Hdfs</span>.write(fs,
      path = path, 
      overwrite = <span class="hljs-literal">false</span>, <span class="hljs-comment">//will fail if the path already exists</span>
      replication = <span class="hljs-number">4</span>, 
      bufferSize = <span class="hljs-number">4096</span>,
      blockSize =  <span class="hljs-number">134217728</span>, 
      lineSeparator = <span class="hljs-string">"\n"</span>) <span class="hljs-comment">//each written element would include the specified line separator </span>
</code></pre>
<h3><a class="anchor" aria-hidden="true" id="appender"></a><a href="#appender" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Appender</h3>
<p>Finally, the hdfs connector also exposes an <em>append</em> operation which is very similar to <em>writer</em> implementation,
but in this case the materialized <code>Long</code> value only represents the size of the appended data, but not of the whole file.</p>
<p>On the other hand, this method does not allow to configure neither the <em>replication factor</em> nor <em>block size</em> and so on, this is because
these configurations are only set whenever a file is created but an append operation would reuse them from the existing file.</p>
<p>See below an example:</p>
<pre><code class="hljs css language-scala"><span class="hljs-keyword">val</span> hadoopConf = <span class="hljs-keyword">new</span> <span class="hljs-type">Configuration</span>() 
<span class="hljs-comment">//enables the append operation</span>
hadoopConf.setBoolean(<span class="hljs-string">"dfs.support.append"</span>, <span class="hljs-literal">true</span>)
<span class="hljs-comment">//found it necessary when running tests on hadoop mini-cluster, but you should tweak the hadoopConf accordingly to your use case</span>
hadoopConf.set(<span class="hljs-string">"dfs.client.block.write.replace-datanode-on-failure.policy"</span>, <span class="hljs-string">"NEVER"</span>) 
hadoopConf.set(<span class="hljs-string">"fs.default.name"</span>, <span class="hljs-string">s"hdfs://localhost:<span class="hljs-subst">$port</span>"</span>) <span class="hljs-comment">//especifies the local endpoint of the test hadoop minicluster</span>
<span class="hljs-keyword">val</span> fs: <span class="hljs-type">FileSystem</span> = <span class="hljs-type">FileSystem</span>.get(conf)

<span class="hljs-comment">//note that we are re-using the `destinationPath` of the last example since should already exist</span>
<span class="hljs-keyword">val</span> hdfsAppender: <span class="hljs-type">Consumer</span>[<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>], <span class="hljs-type">Task</span>[<span class="hljs-type">Long</span>]] = <span class="hljs-type">Hdfs</span>.append(fs, destinationPath) 
<span class="hljs-keyword">val</span> ob: <span class="hljs-type">Observer</span>[<span class="hljs-type">Array</span>[<span class="hljs-type">Byte</span>]] = ???
<span class="hljs-keyword">val</span> t: <span class="hljs-type">Task</span>[<span class="hljs-type">Long</span>] = ob.consumeWith(hdfsAppender) 
</code></pre>
<h2><a class="anchor" aria-hidden="true" id="local-testing"></a><a href="#local-testing" aria-hidden="true" class="hash-link"><svg class="hash-link-icon" aria-hidden="true" height="16" version="1.1" viewBox="0 0 16 16" width="16"><path fill-rule="evenodd" d="M4 9h1v1H4c-1.5 0-3-1.69-3-3.5S2.55 3 4 3h4c1.45 0 3 1.69 3 3.5 0 1.41-.91 2.72-2 3.25V8.59c.58-.45 1-1.27 1-2.09C10 5.22 8.98 4 8 4H4c-.98 0-2 1.22-2 2.5S3 9 4 9zm9-3h-1v1h1c1 0 2 1.22 2 2.5S13.98 12 13 12H9c-.98 0-2-1.22-2-2.5 0-.83.42-1.64 1-2.09V6.25c-1.09.53-2 1.84-2 3.25C6 11.31 7.55 13 9 13h4c1.45 0 3-1.69 3-3.5S14.5 6 13 6z"></path></svg></a>Local testing</h2>
<p><em>Apache Hadoop</em> has a sub project called <a href="https://mvnrepository.com/artifact/org.apache.hadoop/hadoop-minicluster">Mini Cluster</a>
that allows to locally spin up a single-node Hadoop cluster without the need to set any environment variables or manage hadoop configuration files.</p>
<p>Add to your library dependencies with the desired version:</p>
<pre><code class="hljs css language-scala"><span class="hljs-string">"org.apache.hadoop"</span> % <span class="hljs-string">"hadoop-minicluster"</span> % <span class="hljs-string">"VERSION"</span> % <span class="hljs-type">Test</span>
</code></pre>
<p>From there on, as in this case the tests won't depend on a docker container but will depend the emulation running in the JVM,
you will have to start and stop the hadoop mini cluster from the same test, as a good practice using <code>BeforeAndAfterAll</code>:</p>
<pre><code class="hljs css language-scala"><span class="hljs-keyword">import</span> java.io.<span class="hljs-type">File</span>

<span class="hljs-keyword">import</span> org.apache.hadoop.conf.<span class="hljs-type">Configuration</span>
<span class="hljs-keyword">import</span> org.apache.hadoop.fs.{<span class="hljs-type">FileSystem</span>, <span class="hljs-type">Path</span>}
<span class="hljs-keyword">import</span> org.apache.hadoop.hdfs.{<span class="hljs-type">HdfsConfiguration</span>, <span class="hljs-type">MiniDFSCluster</span>}

<span class="hljs-keyword">private</span> <span class="hljs-keyword">var</span> miniHdfs: <span class="hljs-type">MiniDFSCluster</span> = _
<span class="hljs-keyword">private</span> <span class="hljs-keyword">val</span> dir = <span class="hljs-string">"./temp/hadoop"</span> <span class="hljs-comment">//sample base dir where test data will be stored</span>
<span class="hljs-keyword">private</span> <span class="hljs-keyword">val</span> port: <span class="hljs-type">Int</span> = <span class="hljs-number">54310</span> 
<span class="hljs-keyword">private</span> <span class="hljs-keyword">val</span> conf = <span class="hljs-keyword">new</span> <span class="hljs-type">Configuration</span>()
conf.set(<span class="hljs-string">"fs.default.name"</span>, <span class="hljs-string">s"hdfs://localhost:<span class="hljs-subst">$port</span>"</span>)
conf.setBoolean(<span class="hljs-string">"dfs.support.append"</span>, <span class="hljs-literal">true</span>)

<span class="hljs-keyword">override</span> <span class="hljs-keyword">protected</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">beforeAll</span></span>(): <span class="hljs-type">Unit</span> = {
  <span class="hljs-keyword">val</span> baseDir: <span class="hljs-type">File</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">File</span>(dir, <span class="hljs-string">"test"</span>)
  <span class="hljs-keyword">val</span> miniDfsConf: <span class="hljs-type">HdfsConfiguration</span> = <span class="hljs-keyword">new</span> <span class="hljs-type">HdfsConfiguration</span>
  miniDfsConf.set(<span class="hljs-type">MiniDFSCluster</span>.<span class="hljs-type">HDFS_MINIDFS_BASEDIR</span>, baseDir.getAbsolutePath)
  miniHdfs = <span class="hljs-keyword">new</span> <span class="hljs-type">MiniDFSCluster</span>.<span class="hljs-type">Builder</span>(miniDfsConf)
    .nameNodePort(port)
    .format(<span class="hljs-literal">true</span>)
    .build()
  miniHdfs.waitClusterUp()
}

<span class="hljs-keyword">override</span> <span class="hljs-keyword">protected</span> <span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">afterAll</span></span>(): <span class="hljs-type">Unit</span> = {
  fs.close()
  miniHdfs.shutdown()
}
</code></pre>
</span></div></article></div><div class="docs-prevnext"><a class="docs-prev button" href="/docs/gcs"><span class="arrow-prev">← </span><span>Google Cloud Storage</span></a><a class="docs-next button" href="/docs/parquet"><span>Apache Parquet</span><span class="arrow-next"> →</span></a></div></div></div><nav class="onPageNav"><ul class="toc-headings"><li><a href="#introduction">Introduction</a></li><li><a href="#dependency">Dependency</a></li><li><a href="#getting-started">Getting started</a><ul class="toc-headings"><li><a href="#configuration">Configuration</a></li><li><a href="#reader">Reader</a></li><li><a href="#writer">Writer</a></li><li><a href="#appender">Appender</a></li></ul></li><li><a href="#local-testing">Local testing</a></li></ul></nav></div><footer class="nav-footer" id="footer"><hr class="separator"/><section class="copyright">Copyright © 2020-2020 The Monix Connect Developers.</section></footer></div></body></html>