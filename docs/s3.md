---
id: s3
title: AWS S3
---

## Introduction

_AWS Simple Storage Service_ ([S3](https://aws.amazon.com/s3/?nc=sn&loc=0)) is an object storage service that offers industry leading scalability, availability, security and performance.
It allows data storage of any amount of data, commonly used as a data lake for big data applications which can now be easily integrated with _monix_.

This module exposes a wide range of methods for interacting with S3 _buckets_ and _objects_, where the most exciting ones are the multipart _download_ and _upload_,
 since they use the power of reactive streams to deal with objects of any size in smaller parts (chunks).
   
## Dependency
 
 Add the following dependency in your _build.sbt_:
 
 ```scala
 libraryDependencies += "io.monix" %% "monix-s3" % "0.5.0"
 ```
 
## Async Client
 
 This connector uses the _underlying_ `S3AsyncClient` from the [java aws sdk](https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/services/s3/model/package-summary.html),
 it will allow us to authenticate and create a _non blocking_ channel between our application and the _AWS S3 service_. 
 
 There are different ways to create the connection, all available from the singleton object `monix.connect.s3.S3`. It is explained in more detail in the following sub-sections:
 
 ### From config
  
  The laziest and best way to create the _S3_ connection is to do it from configuration file.
  To do so, you'd just need to create an `application.conf` following the template [reference.conf](https://github.com/monix/monix-connect/tree/master/aws-auth/src/main/resources/reference.conf).
  If you don't create your config file, it will default to the `reference.conf` mentioned just before, which is using `DefaultCredentialsProvider`.
    
    
  Below snippet shows an example of the configuration file to authenticate via `StaticCredentialsProvider` and region `EU-WEST-1`, as you can appreciate, the 
  _http client_ settings are commented out since they are optional, however they could have been specified too for a more grained configuration of the underlying `NettyNioAsyncHttpClient`.
    
```hocon
{
  monix-aws: {
    credentials {
      // [anonymous, default, environment, instance, system, profile, static]
      provider: "static" 

      // optional - only required with static credentials
      static {
        access-key-id: "TESTKEY"      
        secret-access-key: "TESTSECRET"
        session-token: ""
      }
    }
  
    // [ap-south-1, us-gov-east-1, af-south-1, eu-west-2, ...]
    region: "eu-west-1"

    // optional
    #endpoint: ""

    // optional
    # http-client: {
    #   max-concurrency: 10
    #   max-pending-connection-acquires: 1000
    #   connection-acquisition-timeout: 2 minutes
    #   connection-time-to-live: 1 minute
    #   use-idle-connection-reaper: false
    #   read-timeout: 100 seconds
    #   write-timeout: 100 seconds
    # }
  }
}
```

This config file needs to be placed in the `resource` folder, 
therefore it will be automatically picked up from the method call `S3.fromConfig`, which will return a `cats.effect.Resource[Task, S3]`.
The [resource](https://typelevel.org/cats-effect/datatypes/resource.html) will be responsible of the *creation* and *release* of the _S3 connection_. 
The best way of using it is to make it transparent in your application and directly expect an instance of _S3_ in your methods and classes, which will be called from within the 
_usage_ of the _Resource_. See below code snippet to understand the concept:

```scala
 import monix.connect.s3.S3
 import monix.eval.Task
 import software.amazon.awssdk.services.s3.model.NoSuchKeyException

 val bucket = "my-bucket"
 val key = "my-key"
 val content = "my-content"
 
 def runS3App(s3: S3): Task[Array[Byte]] = {
   for {
     _ <- s3.createBucket(bucket)
     _ <- s3.upload(bucket, key, content.getBytes)
     existsObject <- s3.existsObject(bucket, key)
     download <- {
       if(existsObject) s3.download(bucket, key)
       else Task.raiseError(NoSuchKeyException.builder().build()) 
     }
   } yield download
 }
 
  // the connection gets created and released within the use method and the `S3`
  // instance is directly passed to our application for an easier interoperability
  val f = S3.fromConfig.use(s3 => runS3App(s3)).runToFuture
```  
    
## Create
 
 An alternative to using a config file is to pass the _AWS configurations_ by parameters.
 This is a safe implementation since the method, again, handles the _creation_ and _release_ of the connection with the _cats effect_ 
 `Resource` data type. The example below produce exactly the same result as previously using the _config_ file:
 
 ```scala
 import cats.effect.Resource
 import monix.connect.s3.S3
 import monix.eval.Task
 import software.amazon.awssdk.auth.credentials.{AwsBasicCredentials, StaticCredentialsProvider}
 import software.amazon.awssdk.regions.Region
 
 val s3AccessKey: String = "TESTKEY"
 val s3SecretKey: String = "TESTSECRET"
 val basicAWSCredentials = AwsBasicCredentials.create(s3AccessKey, s3SecretKey)
 val staticCredProvider = StaticCredentialsProvider.create(basicAWSCredentials)

 val s3: Resource[Task, S3] = S3.create(staticCredProvider, Region.AWS_GLOBAL)   
```

## Create Unsafe

Another different alternatively is to just pass an already created instance of a `software.amazon.awssdk.services.s3.S3AsyncClient`, which in that case, 
the return type would be directly `S3`, so there won't be no need to deal with `Resource`. 
As the same tittle suggests, this is not a pure way of creating an `S3` since it might be possible to pass a malformed 
instance by parameter or just one that was already released (closed). 

An example:
 
 ```scala
import monix.connect.s3.S3
import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider
import software.amazon.awssdk.services.s3.S3AsyncClient
import software.amazon.awssdk.regions.Region

val s3AsyncClient: S3AsyncClient = S3AsyncClient
  .builder()
  .credentialsProvider(DefaultCredentialsProvider.create())
  .region(Region.EU_CENTRAL_1)
  .build()

val s3: S3 = S3.createUnsafe(s3AsyncClient)
```

Notice that `createUnsafe` an overloaded method that also has just another variant that excepts settings values:

 ```scala
import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider
import software.amazon.awssdk.regions.Region
import monix.connect.s3.S3

val bucket: String = "my-bucket" 
val s3: S3 = S3.createUnsafe(DefaultCredentialsProvider.create(), Region.AF_SOUTH_1)
```

## Create bucket 

Once you have configured the client, you would probably need to _create a bucket_:

```scala
import software.amazon.awssdk.services.s3.model.CreateBucketResponse
import monix.connect.s3.S3

val bucket: String = "my-bucket" 
val s3: S3

val t: Task[CreateBucketResponse] = s3.createBucket(bucket)
```

## Delete bucket 

 On the other hand if you want to remove the bucket:
 
 ```scala
import software.amazon.awssdk.services.s3.model.DeleteBucketResponse
import monix.connect.s3.S3

val bucket: String = "my-bucket" 
val s3: S3

val t: Task[DeleteBucketResponse] = s3.deleteBucket(bucket)
```

## Delete object
  
You can also delete an _object_ within the specified _bucket_ with:
 ```scala
import software.amazon.awssdk.services.s3.model.{DeleteObjectResponse, ListObjectsResponse}
import monix.connect.s3.S3

val bucket: String = "my-bucket" 
val key: String = "path/to/my/object.txt" 
val s3: S3

val t: Task[DeleteObjectResponse] = s3.deleteObject(bucket, key)
```

## Exists bucket
  
Check whether the specified bucket exists or not.

 ```scala
import monix.connect.s3.S3

val bucket: String = "my-bucket" 
val s3: S3

val t: Task[Boolean] = s3.existsBucket(bucket)
```


## Exists object
  
Check whether the specified object exists or not.

 ```scala
import monix.connect.s3.S3

val bucket: String = "my-bucket" 
val key: String = "path/to/my/object.txt" 
val s3: S3

val t: Task[Boolean] = s3.existsObject(bucket, key))
```

## List objects
  
Lists the objects within a _bucket_ and _prefix_.

 ```scala
import software.amazon.awssdk.services.s3.model.{DeleteObjectResponse, ListObjectsResponse}
import software.amazon.awssdk.services.s3.model.S3Object
import monix.connect.s3.S3
import monix.reactive.Observable
import monix.reactive.Consumer

val bucket = "my-bucket"
val prefix = s"prefix/to/list/keys/"

// example to calculate the total object's size
val totalSize: Task[Long] = S3.fromConfig.use{s3 =>
  s3.listObjects(bucket, prefix = Some(prefix))
    .consumeWith(Consumer.foldLeft(0)((acc, s3Object) => acc + s3Object.size))
}

// use `maxTotalKeys` to set a limit to the number of keys you want to fetch
val s3ObjectsWithLimit: Observable[S3Object] = 
  S3.createUnsafe(credentialsProvider, region)
    .listObjects(bucket, maxTotalKeys = Some(1011), prefix = Some(prefix))
```

Listing a very long list of objects might take time and and cause the underlying netty connection to error because 
of the request timeout, in order increase that see fix that you can increase those timeouts with a new configuration for `software.amazon.awssdk.http.nio.netty.NettyNioAsyncHttpClient`
that will be added to the `software.amazon.awssdk.services.s3.S3AsyncClient`. _See_ an example _Async Client_ [section](##Async Client)


## Download
  
Downloads the specified object in a single request as _byte array_. 

 ```scala
import software.amazon.awssdk.services.s3.model.PutObjectResponse
import monix.connect.s3.S3
import monix.eval.Task

val bucket = "my-bucket"
val key = "sample/download/path/file.txt"
val content: Array[Byte] = "Dummy file content".getBytes()
val s3: S3

val t: Task[Array[Byte]] = s3.download("my-bucket", key)
```

Note that this operation is only suitable to be used for small objects,
  for large ones `downloadMultipart` should be used.
  
  On contrast, it might be useful in those cases in which the user 
  only wants to download the first _n_ bytes from an object:
  
```scala
  // only downloads the first 10 bytes
  s3.download("my-bucket", key, firstNBytes = Some(10))
```
  
## Download multipart
  
A method that _safely_ downloads objects of any size by performing _partial download requests_.
The number of bytes to get per each request is specified by the `chunkSize`.

 ```scala
import monix.connect.s3.S3
import monix.reactive.Observable
import monix.reactive.Consumer
val bucket = "my-bucket"
val key = "sample/path/file.json"
val s3: S3

val ob: Observable[Array[Byte]] = s3.downloadMultipart("my-bucket", key, chunkSize = 5242880)
```

## Upload

You can also easily upload an object into an _S3_  with the _upload_ signature.
Note that if you need to update large amount of data you should not be using this method, see instead [multipartUpload](https://connect.monix.io/docs/s3#multipart-upload)`.

 ```scala
import software.amazon.awssdk.services.s3.model.PutObjectResponse
import monix.connect.s3.S3
import monix.eval.Task

val bucket = "my-bucket"
val key = "sample/upload/path/file.txt"
val content: Array[Byte] = "Dummy file content".getBytes()
val s3: S3

val t: Task[PutObjectResponse] = s3.upload("my-bucket", key, content)
```

## Upload multipart

When dealing with large files of data you should use the `multipartUpload` operation.
It is used for consuming an observable of bytes that sends partial upload request of an specific minimum size. 
When the emitted bytes are smallaller than the minimumb request chunk size, they will keep being accumulated until reaching the minimum size mentioned previously. 

This, in comparison with the _single upload_, it reduces substantially the risk on having _OOM_ errors or failed _http requests_ due to the size of the body.

```scala
import software.amazon.awssdk.services.s3.model.CompleteMultipartUploadResponse
import monix.connect.s3.S3
import monix.reactive.Observable
import monix.reactive.Consumer
import monix.eval.Task

val bucket = "my-bucket"
val key = "sample/upload/path/file.txt"
val ob = Observable[Array[Byte]] // preasumably we have a byte array `Obserbable`
val s3: S3

val f = ob.consumeWith(s3.uploadMultipart(bucket, key)).runToFuture
```

Note that the `chunksize` can be specified on the method call, in which the _default_ and _minimum_ value is _5MB_ or _5242880 bytes_.
It also accepts specific _aws configurations_ such as `acl`, `requestPayer`, etc. 


## Local testing

There is actually a very good support on regards to testing `AWS S3` locally, the following sections describes different popular alternatives:
 
### Localstack

 A fully functional local _AWS_ cloud stack available as a docker image.
 
 You would just need to define it as a service in your `docker-compose.yml`:
 
 ```yaml
 localstack:
    image: localstack/localstack:0.11.0
    hostname: localstack
    container_name: localstack
    ports:
      - '4566:4566'
    environment:
      - SERVICES=s3
# very important to specify `s3` on `SERVICES` env var, it would prevent to spin up the rest of the AWS services.
``` 

 Then, execute the following command to build and run the _localstack_ image:
 
 ```shell script
 docker-compose -f ./docker-compose.yml up -d localstack
```

A good point on favor to using _localstack_ is that it provides support for _AWS Anonymous Credentials_, meaning that you can easily connect to your 
local _S3 service_ with no required authentication. 
  
 Below snippet is an example on how to set up the config to be used from `S3.create()`:

 ```hocon
{
  monix-aws: {
    credentials {
      provider: "anonymous"  
    }
    endpoint: "http://localhost:9000"
    region: "us-west-2"
  }
}
```
 
On the other hand, if you prefer to create the client from your app:

 ```scala
import monix.connect.s3.S3
import software.amazon.awssdk.regions.Region
import software.amazon.awssdk.services.s3.S3AsyncClient
import software.amazon.awssdk.auth.credentials.AnonymousCredentialsProvider
import java.net.URI

val localStackEndpoint = "http://localhost:4566"

val s3: S3 = S3.createUnsafe(new AnonymousCredentialsProvider, Region.EU_WEST_1, Some(URI.create(localStackEndpoint)))
 ``` 

_Creating local buckets in localstack_

Whenever you create a bucket on _localstack_, you would better set its _ACL (Access Control List)_ as `public-read` since it might prevent you to encounter [403 access denied](https://github.com/localstack/localstack/issues/406) when reading it back.
If you set the `container_name` field to _localstack_ in `docker-compose.yaml` you can create the bucket and specify the right _ACL_ like:
```shell script
docker exec localstack awslocal s3 mb s3://my-bucket
docker exec localstack awslocal s3api put-bucket-acl --bucket my-bucket --acl public-read
``` 
On the other hand, if prefer to do that from code:

```scala
import monix.connect.s3.S3
import org.scalatest.BeforeAndAfterAll

override def beforeAll() = {
  super.beforeAll()
  S3.createUnsafe(s3AsyncClient).createBucket("my-bucket", acl = Some("public-read")).runSyncUnsafe()
}
```

### Minio
 
[Minio](https://github.com/minio/minio) is another well known docker image that emulates _AWS S3_.

The advantages of using _minio_ over _localstack_ is that it provides a nice _UI_ that allows you to quickly visualize and manage the 
 objects and buckets stored in your local, instead of having to _exec_ in your local image.
 
 On the other hand, a disadvantage could be that it does not support _Anonymous Credentials_, so you have to specify _key_ and _secret_ to create the connection.
 
Add the following service description to your `docker-compose.yaml` file:

```yaml
minio:
  image: minio/minio
  ports:
    - "9000:9000"
  volumes:
    - ./minio/data:/data
  environment:
    - MINIO_ACCESS_KEY=TESTKEY
    - MINIO_SECRET_KEY=TESTSECRET
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
    interval: 35s
    timeout: 20s
    retries: 3
  command: server --compat /data
```

Then, run the following command to build and start the _minio_ image:

```shell script
docker-compose -f ./docker-compose.yml up -d minio
``` 

Check out that the service has started correctly, notice that there is a _healthcheck_ on the definition of the _minio service_, 
that's because it is a heavy image and sometimes it takes bit long to start or it even fails, so by adding it will prevent that to happen.

Finally, now you can already create the connection to _AWS S3_, _notice_ that _minio_ does not support _Anonymous credentials_, instead you'll have to use the _Static Credentials Provider_ and specify the _key_ and _secret_ corresponding respectively to the
 defined environment variables `MINIO_ACCESS_KEY` and `MINIO_SECRET_KEY`.

Below snippet represents the settings that would be needed to locally connect to _minio_ using config file:

 ```hocon
{
  monix-aws: {
    credentials {
      provider: "static"  
      access-key-id: "TESTKEY"
      secret-access-key: "TESTSECRET"    
    }
    endpoint: "http://localhost:9000"
    region: "us-west-2"
  }
}
```

Alternatively you can create connection like:

```scala
import monix.connect.s3.S3
import software.amazon.awssdk.regions.Region
import software.amazon.awssdk.services.s3.S3AsyncClient
import software.amazon.awssdk.auth.credentials.{AwsBasicCredentials, StaticCredentialsProvider}
import java.net.URI

val minioEndPoint: String = "http://localhost:9000"

val s3AccessKey: String = "TESTKEY" //equal to the env var `MINIO_ACCESS_KEY`  
val s3SecretKey: String = "TESTSECRET" //equal to the `env var `MINIO_SECRET_KEY`

val basicAWSCredentials = AwsBasicCredentials.create(s3AccessKey, s3SecretKey)
val staticCredentialsProvider = StaticCredentialsProvider.create(basicAWSCredentials)

val s3: S3 = S3.createUnsafe(basicAWSCredentials, Region.EU_WEST_1, Some(URI.create(minioEndPoint)))
```


### JVM S3 Mock library

In case you prefer to _start_ and _stop_ the _S3_ service from from the code of same test and therefore not depending on _docker_ but just on a _JVM library dependency_, you can refer to [findify/s3Mock](https://github.com/findify/s3mock) to see more. 
