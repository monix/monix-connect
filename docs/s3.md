---
id: s3
title: AWS S3
---

## Introduction

_AWS Simple Storage Service_ ([S3](https://aws.amazon.com/s3/?nc=sn&loc=0)) is an object storage service that offers industry leading scalability, availability, security and performance.
It allows data storage of any amount of data, commonly used as a data lake for big data applications which can now be easily integrated with _monix_.

This module exposes a wide range of methods for interacting with S3 _buckets_ and _objects_, where the most exciting ones are the multipart _download_ and _upload_,
 since they use the power of reactive streams to deal with objects of any size in smaller parts (chunks).
   
## Dependency
 
 Add the following dependency in your _build.sbt_:
 
 ```scala
 libraryDependencies += "io.monix" %% "monix-s3" % "0.6.0"
 ```
 
## Async Client
 
 This connector uses the _underlying_ `S3AsyncClient` from the [java aws sdk](https://sdk.amazonaws.com/java/api/latest/software/amazon/awssdk/services/s3/model/package-summary.html),
 it will allow us to authenticate and create a _non blocking_ channel between our application and the _AWS S3 service_. 
 
 There are different ways to create the connection, all available from the singleton object `monix.connect.s3.S3`. It is explained in more detail in the following sub-sections:
 
 ### From config
  
  The laziest and recommended way to create the _S3_ connection is to do it from configuration file.
  To do so, you'd just need to create an `application.conf` following the template from [reference.conf](https://github.com/monix/monix-connect/tree/master/aws-auth/src/main/resources/reference.conf),
  which also represents the default config in case no additional is provided.
    
  Below snippet shows an example of the configuration file to authenticate via `StaticCredentialsProvider` and region `EU-WEST-1`, as you can appreciate the 
  _http client_ settings are commented out since they are optional, however they could have been specified too for a more fine grained configuration of the underlying `NettyNioAsyncHttpClient`.
    
```hocon
  monix-aws: {
    credentials {
      // [anonymous, default, environment, instance, system, profile, static]
      provider: "static" 

      // optional - only required with static credentials
      static {
        access-key-id: "TESTKEY"      
        secret-access-key: "TESTSECRET"
        session-token: ""
      }
    }
  
    // [ap-south-1, us-gov-east-1, af-south-1, eu-west-2, ...]
    region: "eu-west-1"

    // optional
    #endpoint: ""

    // optional
    # http-client: {
    #   max-concurrency: 10
    #   max-pending-connection-acquires: 1000
    #   connection-acquisition-timeout: 2 minutes
    #   connection-time-to-live: 1 minute
    #   use-idle-connection-reaper: false
    #   read-timeout: 100 seconds
    #   write-timeout: 100 seconds
    # }
  }
```

This config file should be placed in the `resources` folder, therefore it will be automatically picked up from the method call `S3.fromConfig`, which will return a `cats.effect.Resource[Task, S3]`.
The [resource](https://typelevel.org/cats-effect/datatypes/resource.html) is responsible of the *creation* and *release* of the _S3 client_. 

**Try to reuse** the created **S3** client as much as possible in your application multiple times in your application. Otherwise, creating it
multiple times will waste precious resources... See below code snippet to understand the concept:


```scala
 import monix.connect.s3.S3
 import monix.eval.Task
 import software.amazon.awssdk.services.s3.model.NoSuchKeyException
 import pureconfig.KebabCase

 val bucket = "my-bucket"
 val key = "my-key"
 val content = "my-content"
 
 def runS3App(s3: S3): Task[Array[Byte]] = {
   for {
     _ <- s3.createBucket(bucket)
     _ <- s3.upload(bucket, key, content.getBytes)
     existsObject <- s3.existsObject(bucket, key)
     download <- {
       if(existsObject) s3.download(bucket, key)
       else Task.raiseError(NoSuchKeyException.builder().build()) 
     }
   } yield download
 }


  // It allows to specify the [[pureconfig.NamingConvention]]
  // from its argument, by default it uses the [[KebabCase]].  
  val f = S3.fromConfig(KebabCase).use(s3 => runS3App(s3)).runToFuture
  // The connection got created and released within the use method and the `S3`
  // instance is directly passed and should be reused across our application
```  

There is an alternative way to use `fromConfig` which is to load the config first and then passing it to
the method. The difference is that in this case we will be able to override a specific configuration 
from the code, whereas before we were reading it and creating the client straight after.

```scala
 import monix.connect.aws.auth.MonixAwsConf
 import monix.connect.s3.S3
 import monix.eval.Task
 import software.amazon.awssdk.services.s3.model.NoSuchKeyException
 import pureconfig.KebabCase
 import software.amazon.awssdk.regions.Region

 val f = MonixAwsConf.load(KebabCase).memoizeOnSuccess.flatMap{ awsConf =>
   val updatedAwsConf = awsConf.copy(region = Region.EU_CENTRAL_1)
   S3.fromConfig(updatedAwsConf).use(s3 => runS3App(s3))
 }.runToFuture
```

## Create
 
 On the other hand, one can pass the _AWS configurations_ by parameters, and that is safe since the method also handles the _acquisition_ and _release_ of the connection with
 `Resource`. The example below produce exactly the same result as previously using the _config_ file:
 
 ```scala
 import cats.effect.Resource
 import monix.connect.s3.S3
 import monix.eval.Task
 import software.amazon.awssdk.auth.credentials.{AwsBasicCredentials, StaticCredentialsProvider}
 import software.amazon.awssdk.regions.Region
 
 val s3AccessKey: String = "TESTKEY"
 val s3SecretKey: String = "TESTSECRET"
 val basicAWSCredentials = AwsBasicCredentials.create(s3AccessKey, s3SecretKey)
 val staticCredProvider = StaticCredentialsProvider.create(basicAWSCredentials)

 val s3: Resource[Task, S3] = S3.create(staticCredProvider, Region.AWS_GLOBAL)   
```

## Create Unsafe

Another different alternatively is to just pass an already created instance of a `software.amazon.awssdk.services.s3.S3AsyncClient`, which in that case, 
the return type would be directly `S3`, so there won't be no need to deal with `Resource`. 
As the same title suggests, this is not a pure way of creating an `S3` since it might be possible to pass a malformed 
instance by parameter or just one that was already released (closed). 

An example:
 
 ```scala
import monix.connect.s3.S3
import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider
import software.amazon.awssdk.services.s3.S3AsyncClient
import software.amazon.awssdk.regions.Region

val s3AsyncClient: S3AsyncClient = S3AsyncClient
  .builder()
  .credentialsProvider(DefaultCredentialsProvider.create())
  .region(Region.EU_CENTRAL_1)
  .build()

val s3: S3 = S3.createUnsafe(s3AsyncClient)
```

Notice that `createUnsafe` an overloaded method that also has just another variant that excepts settings values:

 ```scala
import software.amazon.awssdk.auth.credentials.DefaultCredentialsProvider
import software.amazon.awssdk.regions.Region
import monix.connect.s3.S3

val bucket: String = "my-bucket" 
val s3: S3 = S3.createUnsafe(DefaultCredentialsProvider.create(), Region.AF_SOUTH_1)
```

## Create bucket 

Once you have configured the client, you would probably need to _create a bucket_:

```scala
import software.amazon.awssdk.services.s3.model.CreateBucketResponse
import monix.connect.s3.S3

val bucket: String = "my-bucket" 
val s3: S3

val t: Task[CreateBucketResponse] = s3.createBucket(bucket)
```

## Delete bucket 

 On the other hand if you want to remove the bucket:
 
 ```scala
import software.amazon.awssdk.services.s3.model.DeleteBucketResponse
import monix.connect.s3.S3

val bucket: String = "my-bucket" 
val s3: S3

val t: Task[DeleteBucketResponse] = s3.deleteBucket(bucket)
```

## Delete object
  
You can also delete an _object_ within the specified _bucket_ with:
 ```scala
import software.amazon.awssdk.services.s3.model.{DeleteObjectResponse, ListObjectsResponse}
import monix.connect.s3.S3

val bucket: String = "my-bucket" 
val key: String = "path/to/my/object.txt" 
val s3: S3

val t: Task[DeleteObjectResponse] = s3.deleteObject(bucket, key)
```

## Exists bucket
  
Check whether the specified bucket exists or not.

 ```scala
import monix.connect.s3.S3

val bucket: String = "my-bucket" 
val s3: S3

val t: Task[Boolean] = s3.existsBucket(bucket)
```


## Exists object
  
Check whether the specified object exists or not.

 ```scala
import monix.connect.s3.S3

val bucket: String = "my-bucket" 
val key: String = "path/to/my/object.txt" 
val s3: S3

val t: Task[Boolean] = s3.existsObject(bucket, key))
```

## List objects
  
Lists the objects within a _bucket_ and _prefix_.

 ```scala
import monix.connect.s3.S3
import monix.eval.Task
import monix.reactive.Consumer
import monix.reactive.Observable
import software.amazon.awssdk.services.s3.model.{DeleteObjectResponse, ListObjectsResponse}
import software.amazon.awssdk.services.s3.model.S3Object

val bucket = "my-bucket"
val prefix = "prefix/to/list/keys/"

// example to calculate the total object's size
val totalSize: Task[Long] = S3.fromConfig.use{ s3 =>
  s3.listObjects(bucket, prefix = Some(prefix))
    .consumeWith(Consumer.foldLeft(0)((acc, s3Object) => acc + s3Object.size))
}

// use `maxTotalKeys` to set a limit to the number of keys you want to fetch
val s3ObjectsWithLimit: Observable[S3Object] = 
  S3.createUnsafe(credentialsProvider, region)
    .listObjects(bucket, maxTotalKeys = Some(1011), prefix = Some(prefix))
```

Listing a very long list of objects can take long, and it might cause the underlying _netty_ connection to error due to
a request timeout. 
In order avoid that, you could set a smaller maximum total of keys fetched, or alternatively increase the timeouts
setting on `software.amazon.awssdk.http.nio.netty.NettyNioAsyncHttpClient`,
which will be added to the `software.amazon.awssdk.services.s3.S3AsyncClient`, you can also change that from the _.conf_ file.

### List latest and oldest objects

This connector also exposes a variant for listing the _latest_ and _oldest_ objects under a given path.
They might be handy in those cases where for instance, there is a list of files under
the same path prefix , but we are only interested in the latest updated one,
in such case you could just make use of `listLatestObject` or `listLatestNObjects`.
On the other hand if the interest is on the oldest objects, `listOldestObject` or `listOldestNObjects` would be the appropriate choice.

## Download
Downloads the specified object in a single request as _byte array_. 

 ```scala
import software.amazon.awssdk.services.s3.model.PutObjectResponse
import monix.connect.s3.S3
import monix.eval.Task

val bucket = "my-bucket"
val key = "sample/download/path/file.txt"
val s3: S3

val t: Task[Array[Byte]] = s3.download("my-bucket", key)
```

Note that this operation is only suitable to be used for small objects,
  for large ones `downloadMultipart` should be used.
  
  On contrast, it might be useful in those cases in which the user 
  only wants to download the first _n_ bytes from an object:
  
```scala
  // only downloads the first 10 bytes
  s3.download("my-bucket", key, firstNBytes = Some(10))
```
  
## Download multipart
  
A method that _safely_ downloads objects of any size by performing _partial download requests_.
The number of bytes to get per each request is specified by the `chunkSize`.

 ```scala
import monix.connect.s3.S3
import monix.reactive.Observable
import monix.reactive.Consumer
val bucket = "my-bucket"
val key = "sample/path/file.json"
val s3: S3

// the minimum configurable chunksize is 5MB, although the last chunk might be of smaller size
val ob: Observable[Array[Byte]] = s3.downloadMultipart("my-bucket", key, chunkSize = 5242880)
```

## Upload

You can also easily upload an object into an _S3_  with the _upload_ signature.
Note that if you need to update large amount of data you should not be using this method, see instead [multipartUpload](https://connect.monix.io/docs/s3#multipart-upload)`.

 ```scala
import software.amazon.awssdk.services.s3.model.PutObjectResponse
import monix.connect.s3.S3
import monix.eval.Task

val bucket = "my-bucket"
val key = "sample/upload/path/file.txt"
val content: Array[Byte] = "Dummy file content".getBytes()
val s3: S3

val t: Task[PutObjectResponse] = s3.upload("my-bucket", key, content)
```

## Upload multipart

When dealing with large files of data you should use the `multipartUpload` operation.
It is used for consuming an observable of bytes that sends partial upload request of an specific minimum size. 
When the emitted bytes are smallaller than the minimumb request chunk size, they will keep being accumulated until reaching the minimum size mentioned previously. 

In comparison with the _single upload_, it reduces substantially the risk on getting _OOM_ errors or failed _http requests_ due to the size of the body.

```scala
import software.amazon.awssdk.services.s3.model.CompleteMultipartUploadResponse
import monix.connect.s3.S3
import monix.reactive.Observable
import monix.reactive.Consumer
import monix.eval.Task

val bucket = "my-bucket"
val key = "sample/upload/path/file.txt"
val ob = Observable[Array[Byte]] // preasumably we have a byte array `Observable`
val s3: S3

val f = ob.consumeWith(s3.uploadMultipart(bucket, key)).runToFuture
```

Note that the `chunksize` can be specified on the method call, in which the _default_ and _minimum_ value is _5MB_  (_5242880 bytes_).
It also accepts specific _aws configurations_ such as `acl`, `requestPayer`, etc. 


## Local testing

There is actually a very good support on regards to testing `AWS S3` locally, the following sections describes different popular alternatives:
 
### Localstack

 A fully functional local _AWS_ cloud stack available as a docker image.
 
 You would just need to define it as a service in your `docker-compose.yml`:
 
 ```yaml
 localstack:
    image: localstack/localstack:0.11.0
    hostname: localstack
    container_name: localstack
    ports:
      - '4566:4566'
    environment:
      - SERVICES=s3
# very important to specify `s3` on `SERVICES` env var, it would prevent to spin up the rest of the AWS services.
``` 

 Then, execute the following command to build and run the _localstack_ image:
 
 ```shell script
 docker-compose -f ./docker-compose.yml up -d localstack
```

A good point on favor to using _localstack_ is that it provides support for _AWS Anonymous Credentials_, meaning that you can easily connect to your 
local _S3 service_ with no required authentication. 
  
 Below snippet is an example on how to set up the config to be used from `S3.create()`:

 ```hocon
{
  monix-aws: {
    credentials {
      provider: "anonymous"  
    }
    endpoint: "http://localhost:4566"
    region: "us-west-2"
  }
}
```
 
On the other hand, if you prefer to create the client from your app:

 ```scala
import monix.connect.s3.S3
import software.amazon.awssdk.regions.Region
import software.amazon.awssdk.services.s3.S3AsyncClient
import software.amazon.awssdk.auth.credentials.AnonymousCredentialsProvider
import java.net.URI

val localStackEndpoint = "http://localhost:4566"

val s3: S3 = S3.createUnsafe(new AnonymousCredentialsProvider, Region.EU_WEST_1, Some(URI.create(localStackEndpoint)))
 ``` 

_Creating local buckets in localstack_

Whenever you create a bucket on _localstack_, you would better set its _ACL (Access Control List)_ as `public-read` since it might prevent you to encounter [403 access denied](https://github.com/localstack/localstack/issues/406) when reading it back.
If you set the `container_name` field to _localstack_ in `docker-compose.yaml` you can create the bucket and specify the right _ACL_ like:
```shell script
docker exec localstack awslocal s3 mb s3://my-bucket
docker exec localstack awslocal s3api put-bucket-acl --bucket my-bucket --acl public-read
``` 
On the other hand, if prefer to do that from code:

```scala
import monix.connect.s3.S3
import org.scalatest.BeforeAndAfterAll

override def beforeAll() = {
  super.beforeAll()
  S3.createUnsafe(s3AsyncClient).createBucket("my-bucket", acl = Some("public-read")).runSyncUnsafe()
}
```

### Minio
 
[Minio](https://github.com/minio/minio) is another well known docker image that emulates _AWS S3_.

The advantages of using _minio_ over _localstack_ is that it provides a nice _UI_ that allows you to quickly visualize and manage the 
 objects and buckets stored in your local, instead of having to _exec_ in your local image.
 
 On the other hand, a disadvantage could be that it does not support _Anonymous Credentials_, so you have to specify _key_ and _secret_ to create the connection.
 
Add the following service description to your `docker-compose.yaml` file:

```yaml
minio:
  image: minio/minio
  ports:
    - "9000:9000"
  volumes:
    - ./minio/data:/data
  environment:
    - MINIO_ACCESS_KEY=TESTKEY
    - MINIO_SECRET_KEY=TESTSECRET
  healthcheck:
    test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
    interval: 35s
    timeout: 20s
    retries: 3
  command: server --compat /data
```

Then, run the following command to build and start the _minio_ image:

```shell script
docker-compose -f ./docker-compose.yml up -d minio
``` 

Check out that the service has started correctly, notice that there is a _healthcheck_ on the definition of the _minio service_, 
that's because it is a heavy image and sometimes it takes bit long to start or it even fails, so by adding it will prevent that to happen.

Finally, now you can already create the connection to _AWS S3_, _notice_ that _minio_ does not support _Anonymous credentials_, instead you'll have to use the _Static Credentials Provider_ and specify the _key_ and _secret_ corresponding respectively to the
 defined environment variables `MINIO_ACCESS_KEY` and `MINIO_SECRET_KEY`.

Below snippet represents the settings that would be needed to locally connect to _minio_ using config file, which should be placed
under your test resources folder `src/test/resources`.


 ```hocon
{
  monix-aws: {
    credentials {
      provider: "static"  
      access-key-id: "TESTKEY"
      secret-access-key: "TESTSECRET"    
    }
    endpoint: "http://localhost:9000"
    region: "us-west-2"
  }
}
```

Alternatively you can statically create connection by parameters:

```scala
import monix.connect.s3.S3
import software.amazon.awssdk.regions.Region
import software.amazon.awssdk.services.s3.S3AsyncClient
import software.amazon.awssdk.auth.credentials.{AwsBasicCredentials, StaticCredentialsProvider}
import java.net.URI

val minioEndPoint: String = "http://localhost:9000"

val s3AccessKey: String = "TESTKEY" //equal to the env var `MINIO_ACCESS_KEY`  
val s3SecretKey: String = "TESTSECRET" //equal to the `env var `MINIO_SECRET_KEY`

val basicAWSCredentials = AwsBasicCredentials.create(s3AccessKey, s3SecretKey)
val staticCredentialsProvider = StaticCredentialsProvider.create(basicAWSCredentials)

val s3Resource = S3.create(basicAWSCredentials, Region.EU_WEST_1, Some(URI.create(minioEndPoint)))
```


### JVM S3 Mock library

In case you prefer to _start_ and _stop_ the _S3_ service from the code of same test and therefore not depending on _docker_ but just on a _JVM library dependency_, you can refer to [findify/s3Mock](https://github.com/findify/s3mock) to find out more. 
